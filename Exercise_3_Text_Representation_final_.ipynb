{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "tws",
      "language": "python",
      "name": "tws"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Exercise 3. Text Representation_final...ipynb",
      "provenance": [],
      "collapsed_sections": [
        "k5ZGsi1EqgFR",
        "A6OuQKV8qgFc",
        "LV24_geTqgFp",
        "LsrBTg00qgFu",
        "trkirx8pqgF4",
        "OamWxN8nqgGJ",
        "pbznowX2qgHD",
        "AFhhHtsoqgHS",
        "Ba1boL1bqgHZ"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lupis30puc/bipm_text_analytics_exercises/blob/main/Exercise_3_Text_Representation_final_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lnb-COUqgDK"
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.preprocessing import Binarizer\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import TfidfModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uy7lVE7sqgDR"
      },
      "source": [
        "### 2. Load stemmed dataset from exercise 2:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzrbaXmDqgDS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "outputId": "afe14e1b-6b12-470e-8bc0-e7f068dd0c59"
      },
      "source": [
        "stem_corpus = pickle.load(open('stem_corpus.pkl','rb'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a84986851b3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstem_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stem_corpus.pkl'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'stem_corpus.pkl'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-nVfjyVqgDX",
        "outputId": "1e8dcc90-5f03-4d13-b0e1-3d2df8f8d5e4"
      },
      "source": [
        "stem_corpus.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>target</th>\n",
              "      <th>target_names</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>car wonder enlighten car saw dai door sport ca...</td>\n",
              "      <td>7</td>\n",
              "      <td>rec.autos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>clock poll final final clock report acceler cl...</td>\n",
              "      <td>4</td>\n",
              "      <td>comp.sys.mac.hardware</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>question folk mac plu final gave ghost weekend...</td>\n",
              "      <td>4</td>\n",
              "      <td>comp.sys.mac.hardware</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>weitek nntppostinghost amber ssd csd harri com...</td>\n",
              "      <td>1</td>\n",
              "      <td>comp.graphics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>shuttl launch question articl cowcb world std ...</td>\n",
              "      <td>14</td>\n",
              "      <td>sci.space</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             content  target  \\\n",
              "0  car wonder enlighten car saw dai door sport ca...       7   \n",
              "1  clock poll final final clock report acceler cl...       4   \n",
              "2  question folk mac plu final gave ghost weekend...       4   \n",
              "3  weitek nntppostinghost amber ssd csd harri com...       1   \n",
              "4  shuttl launch question articl cowcb world std ...      14   \n",
              "\n",
              "            target_names  \n",
              "0              rec.autos  \n",
              "1  comp.sys.mac.hardware  \n",
              "2  comp.sys.mac.hardware  \n",
              "3          comp.graphics  \n",
              "4              sci.space  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SKWBOMJqgDd"
      },
      "source": [
        "corpus = stem_corpus.content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJ2TN2ooqgDi",
        "outputId": "bd39ca4e-9102-41ea-e331-5dab320f8b39"
      },
      "source": [
        "corpus[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'car wonder enlighten car saw dai door sport car look late earli call bricklin door small addit bumper separ rest bodi know tellm model engin spec year product car histori info funki look car email thank brought neighborhood lerxst'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oj57U0wOqgDq",
        "outputId": "7324450e-ed1a-4b70-933a-926b13b1ed3c"
      },
      "source": [
        "stem_corpus.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 11314 entries, 0 to 11313\n",
            "Data columns (total 3 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   content       11314 non-null  object\n",
            " 1   target        11314 non-null  int64 \n",
            " 2   target_names  11314 non-null  object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 673.6+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAcl7uM3qgDv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NflkfTbtqgD4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9VFk7paqgD7"
      },
      "source": [
        "## 3. Bag-of-words (sklearn):\n",
        "### a. Apply the following transformations to the stemmed data using the fit_transform() function. \n",
        "What is each of them doing? \n",
        "What are the pros and cons of each approach?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqtVnJdmqgD8"
      },
      "source": [
        "approach_1 = CountVectorizer(max_df=0.95, min_df=0.05)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTaLlDmKzD0q"
      },
      "source": [
        "#max_df= is to remove words that appear repetitive words\n",
        "#min_df= is to remove words appearing very few times..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wH6SnDqAqgEA"
      },
      "source": [
        "transform_1 = approach_1.fit_transform(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTuTKQhpqgEE",
        "outputId": "d4c38c69-d85b-4f46-da0f-bd4688d2ae42"
      },
      "source": [
        "print(approach_1.get_feature_names()) #all stemmed words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['abl', 'accept', 'access', 'actual', 'address', 'advanc', 'ago', 'agre', 'allow', 'answer', 'anybodi', 'appreci', 'apr', 'area', 'articl', 'ask', 'assum', 'avail', 'awai', 'bad', 'base', 'believ', 'best', 'better', 'big', 'bit', 'book', 'bui', 'call', 'car', 'card', 'care', 'case', 'caus', 'chang', 'check', 'chip', 'christian', 'claim', 'close', 'com', 'come', 'complet', 'consid', 'control', 'correct', 'cost', 'cours', 'current', 'dai', 'data', 'david', 'deal', 'design', 'differ', 'discuss', 'drive', 'edu', 'effect', 'email', 'end', 'engin', 'exampl', 'exist', 'expect', 'experi', 'fact', 'far', 'fax', 'feel', 'file', 'final', 'follow', 'free', 'game', 'gener', 'get', 'given', 'go', 'god', 'good', 'got', 'govern', 'great', 'group', 'guess', 'gui', 'hand', 'happen', 'hard', 'have', 'heard', 'help', 'high', 'home', 'hope', 'human', 'idea', 'import', 'includ', 'info', 'inform', 'interest', 'internet', 'isn', 'issu', 'john', 'kei', 'kill', 'kind', 'know', 'larg', 'law', 'left', 'let', 'life', 'like', 'line', 'list', 'littl', 'live', 'local', 'long', 'look', 'lot', 'machin', 'mail', 'major', 'make', 'man', 'mark', 'matter', 'mayb', 'mean', 'mention', 'messag', 'mind', 'nation', 'need', 'net', 'new', 'nntppostinghost', 'note', 'number', 'old', 'open', 'opinion', 'order', 'origin', 'peopl', 'person', 'phone', 'place', 'plai', 'point', 'possibl', 'post', 'power', 'pretti', 'price', 'probabl', 'problem', 'program', 'provid', 'public', 'question', 'read', 'real', 'reason', 'recent', 'refer', 'rememb', 'report', 'requir', 'research', 'respons', 'result', 'right', 'run', 'sai', 'said', 'sale', 'second', 'seen', 'send', 'set', 'small', 'softwar', 'sound', 'sourc', 'space', 'standard', 'start', 'state', 'stuff', 'suggest', 'support', 'sure', 'system', 'take', 'talk', 'team', 'tell', 'thank', 'thing', 'think', 'thought', 'time', 'todai', 'true', 'try', 'turn', 'type', 'understand', 'univers', 'us', 'usual', 'version', 'view', 'wai', 'want', 'week', 'win', 'window', 'won', 'wonder', 'word', 'work', 'world', 'write', 'wrong', 'wrote', 'xnewsread', 'ye', 'year']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NvoGKcKqgEL",
        "outputId": "8656c273-b5b1-4d41-d7f0-86d1168c422c"
      },
      "source": [
        "print(transform_1[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 29)\t5\n",
            "  (0, 225)\t1\n",
            "  (0, 49)\t1\n",
            "  (0, 123)\t2\n",
            "  (0, 28)\t1\n",
            "  (0, 186)\t1\n",
            "  (0, 110)\t1\n",
            "  (0, 61)\t1\n",
            "  (0, 234)\t1\n",
            "  (0, 100)\t1\n",
            "  (0, 59)\t1\n",
            "  (0, 203)\t1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVjUjFFLqgEQ",
        "outputId": "b0054e4d-de6c-4036-f5a3-4b75ed79a7e3"
      },
      "source": [
        "print(transform_1.toarray())# the matrix: rows are count in each content[n], columns: words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 ... 0 0 1]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 1 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 1 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5xZ98wVqgET"
      },
      "source": [
        "approach_2 = TfidfVectorizer(max_df=0.95, min_df=0.05,use_idf=False, norm='l1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJeWOdaczzMb"
      },
      "source": [
        "You remove it to get relative frequencies, and change to norm : l1 (against norm l2, L squared, this is when you want to amplify things, )\n",
        "\n",
        "it is tricky to modify the idf, it applies to the frequency...\n",
        "The results are different\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-9e5P5RqgEW"
      },
      "source": [
        "transform_2 = approach_2.fit_transform(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ozIlntmqgEZ",
        "outputId": "65052607-20d1-4b19-b46d-ba3d7e2585e8"
      },
      "source": [
        "print(approach_2.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['abl', 'accept', 'access', 'actual', 'address', 'advanc', 'ago', 'agre', 'allow', 'answer', 'anybodi', 'appreci', 'apr', 'area', 'articl', 'ask', 'assum', 'avail', 'awai', 'bad', 'base', 'believ', 'best', 'better', 'big', 'bit', 'book', 'bui', 'call', 'car', 'card', 'care', 'case', 'caus', 'chang', 'check', 'chip', 'christian', 'claim', 'close', 'com', 'come', 'complet', 'consid', 'control', 'correct', 'cost', 'cours', 'current', 'dai', 'data', 'david', 'deal', 'design', 'differ', 'discuss', 'drive', 'edu', 'effect', 'email', 'end', 'engin', 'exampl', 'exist', 'expect', 'experi', 'fact', 'far', 'fax', 'feel', 'file', 'final', 'follow', 'free', 'game', 'gener', 'get', 'given', 'go', 'god', 'good', 'got', 'govern', 'great', 'group', 'guess', 'gui', 'hand', 'happen', 'hard', 'have', 'heard', 'help', 'high', 'home', 'hope', 'human', 'idea', 'import', 'includ', 'info', 'inform', 'interest', 'internet', 'isn', 'issu', 'john', 'kei', 'kill', 'kind', 'know', 'larg', 'law', 'left', 'let', 'life', 'like', 'line', 'list', 'littl', 'live', 'local', 'long', 'look', 'lot', 'machin', 'mail', 'major', 'make', 'man', 'mark', 'matter', 'mayb', 'mean', 'mention', 'messag', 'mind', 'nation', 'need', 'net', 'new', 'nntppostinghost', 'note', 'number', 'old', 'open', 'opinion', 'order', 'origin', 'peopl', 'person', 'phone', 'place', 'plai', 'point', 'possibl', 'post', 'power', 'pretti', 'price', 'probabl', 'problem', 'program', 'provid', 'public', 'question', 'read', 'real', 'reason', 'recent', 'refer', 'rememb', 'report', 'requir', 'research', 'respons', 'result', 'right', 'run', 'sai', 'said', 'sale', 'second', 'seen', 'send', 'set', 'small', 'softwar', 'sound', 'sourc', 'space', 'standard', 'start', 'state', 'stuff', 'suggest', 'support', 'sure', 'system', 'take', 'talk', 'team', 'tell', 'thank', 'thing', 'think', 'thought', 'time', 'todai', 'true', 'try', 'turn', 'type', 'understand', 'univers', 'us', 'usual', 'version', 'view', 'wai', 'want', 'week', 'win', 'window', 'won', 'wonder', 'word', 'work', 'world', 'write', 'wrong', 'wrote', 'xnewsread', 'ye', 'year']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIqRamaHqgEc",
        "outputId": "6439cf08-5030-4a18-b6db-984788690d73"
      },
      "source": [
        "print(transform_2[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 29)\t0.29411764705882354\n",
            "  (0, 225)\t0.058823529411764705\n",
            "  (0, 49)\t0.058823529411764705\n",
            "  (0, 123)\t0.11764705882352941\n",
            "  (0, 28)\t0.058823529411764705\n",
            "  (0, 186)\t0.058823529411764705\n",
            "  (0, 110)\t0.058823529411764705\n",
            "  (0, 61)\t0.058823529411764705\n",
            "  (0, 234)\t0.058823529411764705\n",
            "  (0, 100)\t0.058823529411764705\n",
            "  (0, 59)\t0.058823529411764705\n",
            "  (0, 203)\t0.058823529411764705\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ci96uMVmqgEi",
        "outputId": "16b82296-2a7a-431e-ad23-39c25369a923"
      },
      "source": [
        "print(transform_2.toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.         0.         0.         ... 0.         0.         0.05882353]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.01724138 ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.         0.         0.         ... 0.07142857 0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pp71MOsCqgEl"
      },
      "source": [
        "approach_3 = TfidfVectorizer(max_df=0.95, min_df=0.05, smooth_idf=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FA2SyudM1vbc"
      },
      "source": [
        "Many ways to change the frequencies and the smoothing techniques...\n",
        "\n",
        "What smooth_idf=False does is:\n",
        " idf(t) = log [ n / df(t) ] + 1 \n",
        " This avoids the lose of data even if values appear in all documents. \n",
        "\n",
        " Scikit learn has less options for modifying that gensim..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SB0emEVfqgEo"
      },
      "source": [
        "transform_3 = approach_3.fit_transform(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zgSlot0qgEr",
        "outputId": "c56f8f45-cadc-49b1-d52e-484dd493d0d6"
      },
      "source": [
        "print(approach_3.get_feature_names()) #its the same features!!"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['abl', 'accept', 'access', 'actual', 'address', 'advanc', 'ago', 'agre', 'allow', 'answer', 'anybodi', 'appreci', 'apr', 'area', 'articl', 'ask', 'assum', 'avail', 'awai', 'bad', 'base', 'believ', 'best', 'better', 'big', 'bit', 'book', 'bui', 'call', 'car', 'card', 'care', 'case', 'caus', 'chang', 'check', 'chip', 'christian', 'claim', 'close', 'com', 'come', 'complet', 'consid', 'control', 'correct', 'cost', 'cours', 'current', 'dai', 'data', 'david', 'deal', 'design', 'differ', 'discuss', 'drive', 'edu', 'effect', 'email', 'end', 'engin', 'exampl', 'exist', 'expect', 'experi', 'fact', 'far', 'fax', 'feel', 'file', 'final', 'follow', 'free', 'game', 'gener', 'get', 'given', 'go', 'god', 'good', 'got', 'govern', 'great', 'group', 'guess', 'gui', 'hand', 'happen', 'hard', 'have', 'heard', 'help', 'high', 'home', 'hope', 'human', 'idea', 'import', 'includ', 'info', 'inform', 'interest', 'internet', 'isn', 'issu', 'john', 'kei', 'kill', 'kind', 'know', 'larg', 'law', 'left', 'let', 'life', 'like', 'line', 'list', 'littl', 'live', 'local', 'long', 'look', 'lot', 'machin', 'mail', 'major', 'make', 'man', 'mark', 'matter', 'mayb', 'mean', 'mention', 'messag', 'mind', 'nation', 'need', 'net', 'new', 'nntppostinghost', 'note', 'number', 'old', 'open', 'opinion', 'order', 'origin', 'peopl', 'person', 'phone', 'place', 'plai', 'point', 'possibl', 'post', 'power', 'pretti', 'price', 'probabl', 'problem', 'program', 'provid', 'public', 'question', 'read', 'real', 'reason', 'recent', 'refer', 'rememb', 'report', 'requir', 'research', 'respons', 'result', 'right', 'run', 'sai', 'said', 'sale', 'second', 'seen', 'send', 'set', 'small', 'softwar', 'sound', 'sourc', 'space', 'standard', 'start', 'state', 'stuff', 'suggest', 'support', 'sure', 'system', 'take', 'talk', 'team', 'tell', 'thank', 'thing', 'think', 'thought', 'time', 'todai', 'true', 'try', 'turn', 'type', 'understand', 'univers', 'us', 'usual', 'version', 'view', 'wai', 'want', 'week', 'win', 'window', 'won', 'wonder', 'word', 'work', 'world', 'write', 'wrong', 'wrote', 'xnewsread', 'ye', 'year']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OevvOvVsqgEu",
        "outputId": "88256af4-e058-4ab9-8ae4-219a9002eb12"
      },
      "source": [
        "print(transform_3[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 203)\t0.121682990038635\n",
            "  (0, 59)\t0.13619718354871735\n",
            "  (0, 100)\t0.17360510673829524\n",
            "  (0, 234)\t0.12205830867257342\n",
            "  (0, 61)\t0.17175464030050264\n",
            "  (0, 110)\t0.09716570435591144\n",
            "  (0, 186)\t0.1771717303583916\n",
            "  (0, 28)\t0.1510033429609071\n",
            "  (0, 123)\t0.23634859995373567\n",
            "  (0, 49)\t0.1407339623887973\n",
            "  (0, 225)\t0.16984862582327953\n",
            "  (0, 29)\t0.8508576182722584\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "As5pVnQqqgEx",
        "outputId": "bff7fb36-d950-422b-8d31-0c99a0c09841"
      },
      "source": [
        "print(transform_3.toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.         0.         0.         ... 0.         0.         0.12205831]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.11702349 ... 0.         0.         0.        ]\n",
            " ...\n",
            " [0.         0.         0.         ... 0.27801296 0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7kjqWEQqgE0"
      },
      "source": [
        "How many features are left in the dictionary in iii. and what are their names (Hint: Use get_feature_names())? \n",
        "\n",
        "Do the features make sense? Are they different to those in i. and ii.? \n",
        "\n",
        "What happens if you change max_df and min_df?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXcyncdKqgE1",
        "outputId": "691160f8-06f3-4d39-d5a9-a4e3c9248d73"
      },
      "source": [
        "len(approach_1.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "235"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jk3iDEzGqgE4"
      },
      "source": [
        "#approach_1.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tVx4b4hqgE9",
        "outputId": "6b0e407c-0c8a-4a69-9b16-4e4a096939f7"
      },
      "source": [
        "len(approach_2.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "235"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRMrQn3wqgFD"
      },
      "source": [
        "#approach_2.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S75xHtVJqgFF",
        "outputId": "3951877b-e476-4257-d758-62437e8e59e1"
      },
      "source": [
        "len(approach_3.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "235"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5ZGsi1EqgFR"
      },
      "source": [
        "### b. Apply Binarizer() to the result from a. i. using again fit_transform(). \n",
        "What does the result stand for? What are its pros and cons?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbs5h19qqgFS"
      },
      "source": [
        "#turning floats in the matrix to 1 and 0\n",
        "approach_4 = Binarizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlpF4p3-qgFW"
      },
      "source": [
        "transform_4 = approach_4.fit_transform(transform_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JC9qYMpo3oO_"
      },
      "source": [
        "print"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yk5-xYuPqgFZ",
        "outputId": "7dc57334-9a37-4d0d-8afb-7a398a3e03fb"
      },
      "source": [
        "print(transform_4.toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 ... 0 0 1]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 1 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 1 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6OuQKV8qgFc"
      },
      "source": [
        "### c. Put the frequencies for the first document from a. and b. in a data frame with a column keys containing the feature names from a. and four additional columns with the corresponding frequencies from a. and b. (Hint: Convert the corpus to an array first.).  Keep only those rows of the data frame with keys representing words that exist in the first document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sosiXLLqgFc"
      },
      "source": [
        "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#import pandas as pd\n",
        "\n",
        "#vect = TfidfVectorizer()\n",
        "#tfidf_matrix = vect.fit_transform(documents)\n",
        "#df = pd.DataFrame(tfidf_matrix.toarray(), columns = vect.get_feature_names())\n",
        "#print(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CStp0gipqgFf",
        "outputId": "bb99c650-0370-4ecc-820b-b2eb575669e0"
      },
      "source": [
        "keys = approach_1.get_feature_names()\n",
        "keys = np.asarray(keys) #converting corpus to np.array\n",
        "df_1 = pd.DataFrame(data=keys) #creating the data frame\n",
        "df_1.rename(columns={0 :'Key'}, inplace=True)\n",
        "df_1.set_index('Key', inplace=True) #setting index as key\n",
        "#adding the rest of the data\n",
        "df_1['A1'], df_1['A2'], df_1['A3'], df_1['B'] = [transform_1[0].toarray()[0], transform_2[0].toarray()[0], transform_3[0].toarray()[0], transform_4[0].toarray()[0]]\n",
        "df_1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>A1</th>\n",
              "      <th>A2</th>\n",
              "      <th>A3</th>\n",
              "      <th>B</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Key</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>abl</th>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>accept</th>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>access</th>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>actual</th>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>address</th>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>wrong</th>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>wrote</th>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>xnewsread</th>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ye</th>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>year</th>\n",
              "      <td>1</td>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.122058</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>235 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           A1        A2        A3  B\n",
              "Key                                 \n",
              "abl         0  0.000000  0.000000  0\n",
              "accept      0  0.000000  0.000000  0\n",
              "access      0  0.000000  0.000000  0\n",
              "actual      0  0.000000  0.000000  0\n",
              "address     0  0.000000  0.000000  0\n",
              "...        ..       ...       ... ..\n",
              "wrong       0  0.000000  0.000000  0\n",
              "wrote       0  0.000000  0.000000  0\n",
              "xnewsread   0  0.000000  0.000000  0\n",
              "ye          0  0.000000  0.000000  0\n",
              "year        1  0.058824  0.122058  1\n",
              "\n",
              "[235 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCeC6o_jqgFj",
        "outputId": "bacb66d0-7678-4343-eb8a-5326fdab89c7"
      },
      "source": [
        "#removing rows with 0 occurance \n",
        "df_1 = df_1[(df_1.T != 0).any()]     \n",
        "df_1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>A1</th>\n",
              "      <th>A2</th>\n",
              "      <th>A3</th>\n",
              "      <th>B</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Key</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>call</th>\n",
              "      <td>1</td>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.151003</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>car</th>\n",
              "      <td>5</td>\n",
              "      <td>0.294118</td>\n",
              "      <td>0.850858</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dai</th>\n",
              "      <td>1</td>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.140734</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>email</th>\n",
              "      <td>1</td>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.136197</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>engin</th>\n",
              "      <td>1</td>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.171755</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>info</th>\n",
              "      <td>1</td>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.173605</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>know</th>\n",
              "      <td>1</td>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.097166</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>look</th>\n",
              "      <td>2</td>\n",
              "      <td>0.117647</td>\n",
              "      <td>0.236349</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>small</th>\n",
              "      <td>1</td>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.177172</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>thank</th>\n",
              "      <td>1</td>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.121683</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>wonder</th>\n",
              "      <td>1</td>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.169849</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>year</th>\n",
              "      <td>1</td>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.122058</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        A1        A2        A3  B\n",
              "Key                              \n",
              "call     1  0.058824  0.151003  1\n",
              "car      5  0.294118  0.850858  1\n",
              "dai      1  0.058824  0.140734  1\n",
              "email    1  0.058824  0.136197  1\n",
              "engin    1  0.058824  0.171755  1\n",
              "info     1  0.058824  0.173605  1\n",
              "know     1  0.058824  0.097166  1\n",
              "look     2  0.117647  0.236349  1\n",
              "small    1  0.058824  0.177172  1\n",
              "thank    1  0.058824  0.121683  1\n",
              "wonder   1  0.058824  0.169849  1\n",
              "year     1  0.058824  0.122058  1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujacXIRjqgFn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LV24_geTqgFp"
      },
      "source": [
        "### d. Sort the data frame (descending) on:\n",
        "####         i. TF-IDF Frequencies\n",
        "####         ii. Absolute Frequencies\n",
        "Do you see differences? Why?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJQGFk89qgFq",
        "outputId": "ff6814f7-43be-4b5e-8222-2d80618e6014"
      },
      "source": [
        "#sorting \n",
        "print(df_1.sort_values(by=['A3'], ascending=False))\n",
        "print(df_1.sort_values(by=['A1'], ascending=False))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        A1        A2        A3  B\n",
            "Key                              \n",
            "car      5  0.294118  0.850858  1\n",
            "look     2  0.117647  0.236349  1\n",
            "small    1  0.058824  0.177172  1\n",
            "info     1  0.058824  0.173605  1\n",
            "engin    1  0.058824  0.171755  1\n",
            "wonder   1  0.058824  0.169849  1\n",
            "call     1  0.058824  0.151003  1\n",
            "dai      1  0.058824  0.140734  1\n",
            "email    1  0.058824  0.136197  1\n",
            "year     1  0.058824  0.122058  1\n",
            "thank    1  0.058824  0.121683  1\n",
            "know     1  0.058824  0.097166  1\n",
            "        A1        A2        A3  B\n",
            "Key                              \n",
            "car      5  0.294118  0.850858  1\n",
            "look     2  0.117647  0.236349  1\n",
            "call     1  0.058824  0.151003  1\n",
            "dai      1  0.058824  0.140734  1\n",
            "email    1  0.058824  0.136197  1\n",
            "engin    1  0.058824  0.171755  1\n",
            "info     1  0.058824  0.173605  1\n",
            "know     1  0.058824  0.097166  1\n",
            "small    1  0.058824  0.177172  1\n",
            "thank    1  0.058824  0.121683  1\n",
            "wonder   1  0.058824  0.169849  1\n",
            "year     1  0.058824  0.122058  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjdbhLVHqgFt"
      },
      "source": [
        "## 4. Bag-of-words (gensim):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsrBTg00qgFu"
      },
      "source": [
        "### a. Create a corpus as input for gensim with corpus_gen=[doc.split() for doc in data_stem]. What is this command exactly doing?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiTAFj6VqgFu"
      },
      "source": [
        "corpus_gen=[doc.split() for doc in stem_corpus.content]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0b9pDk8qgFx",
        "outputId": "0b34e622-4bb7-47de-a424-98f438906f68"
      },
      "source": [
        "corpus_gen[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['car',\n",
              " 'wonder',\n",
              " 'enlighten',\n",
              " 'car',\n",
              " 'saw',\n",
              " 'dai',\n",
              " 'door',\n",
              " 'sport',\n",
              " 'car',\n",
              " 'look',\n",
              " 'late',\n",
              " 'earli',\n",
              " 'call',\n",
              " 'bricklin',\n",
              " 'door',\n",
              " 'small',\n",
              " 'addit',\n",
              " 'bumper',\n",
              " 'separ',\n",
              " 'rest',\n",
              " 'bodi',\n",
              " 'know',\n",
              " 'tellm',\n",
              " 'model',\n",
              " 'engin',\n",
              " 'spec',\n",
              " 'year',\n",
              " 'product',\n",
              " 'car',\n",
              " 'histori',\n",
              " 'info',\n",
              " 'funki',\n",
              " 'look',\n",
              " 'car',\n",
              " 'email',\n",
              " 'thank',\n",
              " 'brought',\n",
              " 'neighborhood',\n",
              " 'lerxst']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trkirx8pqgF4"
      },
      "source": [
        "### b. Create a gensim Dictionary() based on the corpus. Call it id2word. Apply to it id2word.filter_extremes(no_below=566, no_above=0.95). What is this method doing?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slLNnnxCqgF5"
      },
      "source": [
        "id2word = Dictionary(corpus_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQisn64VqgF9",
        "outputId": "c7ea0a5d-1d86-4f2a-d696-0e346ac4c855"
      },
      "source": [
        "print(id2word)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dictionary(80476 unique tokens: ['addit', 'bodi', 'bricklin', 'brought', 'bumper']...)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWR9CesFqgGA"
      },
      "source": [
        "id2word.filter_extremes(no_below=566, no_above=0.95)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rghMuw2yqgGE",
        "outputId": "867441c8-7a8a-455b-9d6b-73de7e434c6c"
      },
      "source": [
        "print(id2word)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dictionary(235 unique tokens: ['call', 'car', 'dai', 'email', 'engin']...)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OamWxN8nqgGJ"
      },
      "source": [
        "### c.Apply the following operations. What is each of them doing?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vj6FZ1VuqgGJ",
        "outputId": "1b171c18-a110-41dc-ad90-3ccea697101a"
      },
      "source": [
        "#i.\n",
        "print(id2word.token2id)\n",
        "#token -> tokenId."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'call': 0, 'car': 1, 'dai': 2, 'email': 3, 'engin': 4, 'info': 5, 'know': 6, 'look': 7, 'small': 8, 'thank': 9, 'wonder': 10, 'year': 11, 'answer': 12, 'base': 13, 'card': 14, 'edu': 15, 'experi': 16, 'final': 17, 'gui': 18, 'messag': 19, 'nntppostinghost': 20, 'number': 21, 'report': 22, 'send': 23, 'access': 24, 'actual': 25, 'advanc': 26, 'anybodi': 27, 'better': 28, 'bit': 29, 'expect': 30, 'feel': 31, 'good': 32, 'got': 33, 'great': 34, 'heard': 35, 'help': 36, 'life': 37, 'like': 38, 'line': 39, 'machin': 40, 'mayb': 41, 'new': 42, 'opinion': 43, 'peopl': 44, 'plai': 45, 'post': 46, 'price': 47, 'probabl': 48, 'question': 49, 'read': 50, 'real': 51, 'recent': 52, 'start': 53, 'take': 54, 'time': 55, 'us': 56, 'wai': 57, 'address': 58, 'articl': 59, 'chip': 60, 'com': 61, 'far': 62, 'inform': 63, 'person': 64, 'phone': 65, 'point': 66, 'pretti': 67, 'requir': 68, 'stuff': 69, 'system': 70, 'thing': 71, 'version': 72, 'write': 73, 'wrote': 74, 'xnewsread': 75, 'check': 76, 'mean': 77, 'possibl': 78, 'right': 79, 'set': 80, 'softwar': 81, 'tell': 82, 'understand': 83, 'world': 84, 'ye': 85, 'agre': 86, 'allow': 87, 'apr': 88, 'believ': 89, 'come': 90, 'consid': 91, 'control': 92, 'cost': 93, 'cours': 94, 'follow': 95, 'given': 96, 'govern': 97, 'hand': 98, 'hard': 99, 'hope': 100, 'idea': 101, 'john': 102, 'kill': 103, 'make': 104, 'need': 105, 'power': 106, 'reason': 107, 'result': 108, 'sai': 109, 'second': 110, 'state': 111, 'support': 112, 'todai': 113, 'file': 114, 'sure': 115, 'thought': 116, 'try': 117, 'accept': 118, 'avail': 119, 'correct': 120, 'data': 121, 'exist': 122, 'fact': 123, 'go': 124, 'list': 125, 'long': 126, 'note': 127, 'problem': 128, 'refer': 129, 'said': 130, 'think': 131, 'true': 132, 'appreci': 133, 'chang': 134, 'win': 135, 'bui': 136, 'design': 137, 'mention': 138, 'usual': 139, 'work': 140, 'wrong': 141, 'run': 142, 'want': 143, 'assum': 144, 'case': 145, 'christian': 146, 'david': 147, 'differ': 148, 'end': 149, 'exampl': 150, 'get': 151, 'god': 152, 'guess': 153, 'human': 154, 'kind': 155, 'littl': 156, 'live': 157, 'man': 158, 'mind': 159, 'type': 160, 'complet': 161, 'current': 162, 'kei': 163, 'let': 164, 'major': 165, 'mark': 166, 'old': 167, 'place': 168, 'provid': 169, 'research': 170, 'sourc': 171, 'space': 172, 'team': 173, 'ask': 174, 'high': 175, 'includ': 176, 'sale': 177, 'sound': 178, 'caus': 179, 'gener': 180, 'group': 181, 'happen': 182, 'origin': 183, 'standard': 184, 'abl': 185, 'big': 186, 'deal': 187, 'import': 188, 'program': 189, 'won': 190, 'word': 191, 'ago': 192, 'awai': 193, 'best': 194, 'book': 195, 'care': 196, 'claim': 197, 'close': 198, 'drive': 199, 'interest': 200, 'law': 201, 'rememb': 202, 'respons': 203, 'turn': 204, 'univers': 205, 'view': 206, 'seen': 207, 'matter': 208, 'window': 209, 'order': 210, 'fax': 211, 'free': 212, 'mail': 213, 'left': 214, 'isn': 215, 'game': 216, 'lot': 217, 'local': 218, 'larg': 219, 'week': 220, 'effect': 221, 'public': 222, 'have': 223, 'issu': 224, 'open': 225, 'talk': 226, 'home': 227, 'bad': 228, 'net': 229, 'nation': 230, 'area': 231, 'discuss': 232, 'suggest': 233, 'internet': 234, 'addit': 235, 'bodi': 236, 'bricklin': 237, 'brought': 238, 'bumper': 239, 'door': 240, 'earli': 241, 'enlighten': 242, 'funki': 243, 'histori': 244, 'late': 245, 'lerxst': 246, 'model': 247, 'neighborhood': 248, 'product': 249, 'rest': 250, 'saw': 251, 'separ': 252, 'spec': 253, 'sport': 254, 'tellm': 255}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2TkjTI7qgGQ",
        "outputId": "1c9b7880-a116-4c9f-8667-8945619bec61"
      },
      "source": [
        "#ii. it only prints the keys\n",
        "print(id2word.token2id.keys())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['call', 'car', 'dai', 'email', 'engin', 'info', 'know', 'look', 'small', 'thank', 'wonder', 'year', 'answer', 'base', 'card', 'edu', 'experi', 'final', 'gui', 'messag', 'nntppostinghost', 'number', 'report', 'send', 'access', 'actual', 'advanc', 'anybodi', 'better', 'bit', 'expect', 'feel', 'good', 'got', 'great', 'heard', 'help', 'life', 'like', 'line', 'machin', 'mayb', 'new', 'opinion', 'peopl', 'plai', 'post', 'price', 'probabl', 'question', 'read', 'real', 'recent', 'start', 'take', 'time', 'us', 'wai', 'address', 'articl', 'chip', 'com', 'far', 'inform', 'person', 'phone', 'point', 'pretti', 'requir', 'stuff', 'system', 'thing', 'version', 'write', 'wrote', 'xnewsread', 'check', 'mean', 'possibl', 'right', 'set', 'softwar', 'tell', 'understand', 'world', 'ye', 'agre', 'allow', 'apr', 'believ', 'come', 'consid', 'control', 'cost', 'cours', 'follow', 'given', 'govern', 'hand', 'hard', 'hope', 'idea', 'john', 'kill', 'make', 'need', 'power', 'reason', 'result', 'sai', 'second', 'state', 'support', 'todai', 'file', 'sure', 'thought', 'try', 'accept', 'avail', 'correct', 'data', 'exist', 'fact', 'go', 'list', 'long', 'note', 'problem', 'refer', 'said', 'think', 'true', 'appreci', 'chang', 'win', 'bui', 'design', 'mention', 'usual', 'work', 'wrong', 'run', 'want', 'assum', 'case', 'christian', 'david', 'differ', 'end', 'exampl', 'get', 'god', 'guess', 'human', 'kind', 'littl', 'live', 'man', 'mind', 'type', 'complet', 'current', 'kei', 'let', 'major', 'mark', 'old', 'place', 'provid', 'research', 'sourc', 'space', 'team', 'ask', 'high', 'includ', 'sale', 'sound', 'caus', 'gener', 'group', 'happen', 'origin', 'standard', 'abl', 'big', 'deal', 'import', 'program', 'won', 'word', 'ago', 'awai', 'best', 'book', 'care', 'claim', 'close', 'drive', 'interest', 'law', 'rememb', 'respons', 'turn', 'univers', 'view', 'seen', 'matter', 'window', 'order', 'fax', 'free', 'mail', 'left', 'isn', 'game', 'lot', 'local', 'larg', 'week', 'effect', 'public', 'have', 'issu', 'open', 'talk', 'home', 'bad', 'net', 'nation', 'area', 'discuss', 'suggest', 'internet'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a45vugqIqgGT",
        "outputId": "71388a0e-bc31-423a-c8c2-9dbbb860493a"
      },
      "source": [
        "#iii.\n",
        "print(id2word.dfs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{1: 692, 10: 697, 2: 1334, 7: 2206, 0: 1061, 8: 592, 6: 3524, 4: 668, 11: 2023, 5: 641, 3: 1476, 9: 2040, 17: 575, 22: 603, 20: 2320, 15: 6239, 21: 1179, 16: 628, 23: 741, 19: 640, 14: 701, 13: 820, 12: 766, 18: 595, 49: 1774, 53: 1232, 37: 708, 57: 2197, 42: 2820, 40: 602, 29: 1000, 41: 807, 27: 568, 30: 582, 35: 728, 24: 638, 47: 614, 39: 929, 38: 3840, 52: 627, 48: 1118, 33: 1223, 31: 709, 28: 1218, 34: 964, 32: 2337, 43: 1073, 44: 2552, 56: 2519, 54: 632, 51: 906, 45: 763, 25: 1097, 36: 1627, 26: 652, 46: 1711, 50: 1489, 55: 2771, 61: 3897, 75: 625, 72: 961, 74: 808, 73: 6076, 59: 4995, 60: 580, 62: 953, 69: 571, 67: 652, 68: 734, 66: 1555, 58: 635, 65: 710, 63: 1110, 70: 607, 71: 2071, 64: 1246, 84: 1008, 85: 773, 77: 1484, 83: 732, 81: 667, 76: 636, 79: 1843, 80: 982, 78: 1205, 82: 1212, 110: 839, 101: 956, 88: 2729, 102: 765, 106: 941, 104: 1106, 93: 574, 105: 2183, 92: 912, 97: 838, 108: 686, 111: 1247, 90: 1673, 89: 1426, 99: 780, 112: 968, 86: 661, 98: 716, 100: 770, 94: 1026, 109: 1291, 95: 1209, 103: 656, 107: 1299, 87: 741, 96: 674, 91: 970, 113: 707, 116: 938, 115: 1401, 117: 1624, 114: 813, 118: 667, 126: 1060, 128: 1834, 125: 767, 127: 809, 122: 812, 121: 629, 120: 574, 123: 1134, 119: 864, 130: 1306, 131: 2946, 132: 877, 124: 1313, 129: 695, 135: 634, 134: 933, 133: 677, 137: 574, 140: 2330, 141: 817, 139: 578, 138: 636, 136: 720, 142: 1358, 143: 2352, 147: 802, 149: 968, 148: 1344, 152: 836, 145: 1222, 158: 699, 146: 681, 159: 614, 157: 987, 150: 778, 155: 779, 156: 1110, 154: 576, 153: 591, 144: 641, 160: 606, 151: 1063, 172: 607, 166: 580, 164: 1329, 162: 793, 173: 581, 163: 604, 169: 764, 171: 659, 167: 846, 168: 1033, 170: 617, 165: 619, 161: 621, 177: 584, 175: 720, 178: 697, 176: 1140, 174: 1192, 181: 953, 184: 663, 182: 986, 183: 982, 179: 766, 180: 1136, 189: 1000, 186: 742, 191: 845, 185: 825, 188: 621, 190: 708, 187: 569, 198: 615, 199: 887, 205: 1297, 202: 720, 204: 733, 197: 754, 201: 758, 193: 744, 192: 686, 200: 1134, 195: 659, 203: 761, 206: 626, 194: 1012, 196: 640, 207: 791, 209: 999, 208: 627, 210: 806, 212: 647, 213: 700, 211: 609, 214: 618, 215: 802, 216: 777, 217: 1175, 218: 575, 219: 635, 220: 653, 221: 698, 222: 729, 223: 844, 226: 926, 225: 593, 224: 712, 227: 639, 228: 786, 229: 774, 230: 680, 231: 617, 232: 632, 233: 770, 234: 708}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VevWNrF-qgGW"
      },
      "source": [
        "id2word.dfs?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YApeXPN-qgGY"
      },
      "source": [
        "#Document frequencies: token_id -> how many documents contain this token.\n",
        "#dict(**kwargs) -> new dictionary initialized with the name=value pairs\n",
        " #   in the keyword argument list.  For example:  dict(one=1, two=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fNpx_aqqgGa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PK4bkdlqgGe"
      },
      "source": [
        "#iv. bag of words\n",
        "corpus1=[id2word.doc2bow(doc) for doc in corpus_gen]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZFkfm6SqgGg",
        "outputId": "227c2db8-3a7d-4520-d574-be4f99e81abf"
      },
      "source": [
        "print(corpus1[0])\n",
        "print(type(corpus1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0, 1), (1, 5), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 2), (8, 1), (9, 1), (10, 1), (11, 1)]\n",
            "<class 'list'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87b4GdYeqgGm"
      },
      "source": [
        "#v. \n",
        "corpus2=[[(token[0],(token[1]/sum(n for _, n in doc))) for token in doc]\n",
        "for doc in corpus1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edjSAlkdqgGr",
        "outputId": "c8f6bfbb-9565-497b-ea1c-0a1921a6eb8b"
      },
      "source": [
        "print(corpus2[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0, 0.058823529411764705), (1, 0.29411764705882354), (2, 0.058823529411764705), (3, 0.058823529411764705), (4, 0.058823529411764705), (5, 0.058823529411764705), (6, 0.058823529411764705), (7, 0.11764705882352941), (8, 0.058823529411764705), (9, 0.058823529411764705), (10, 0.058823529411764705), (11, 0.058823529411764705)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Osw_3iV1qgGx"
      },
      "source": [
        "#vi. \n",
        "corpus3=[[(token[0],1) for token in doc] for doc in corpus1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHVF_NGrqgG3",
        "outputId": "173651af-1790-47a9-e369-15d642a023d7"
      },
      "source": [
        "print(corpus3[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-_659FhqgG9"
      },
      "source": [
        "#vii. \n",
        "tfidf=TfidfModel(dictionary=id2word, normalize=True)\n",
        "corpus4=[tfidf[id2word.doc2bow(doc)] for doc in corpus_gen]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZbwVjsSqgHA",
        "outputId": "775e46f9-45a9-44b1-ad6f-fccae8c066db"
      },
      "source": [
        "print(corpus4[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0, 0.146733841685233), (1, 0.866148745091086), (2, 0.13253862095166563), (3, 0.12626749519075897), (4, 0.1754180689543496), (5, 0.17797594275794237), (6, 0.07231482893119194), (7, 0.20270943201189023), (8, 0.18290603656499477), (9, 0.10620472906785529), (10, 0.17278341187493057), (11, 0.10672352675158762)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbznowX2qgHD"
      },
      "source": [
        "### d. Put all the frequencies from iv. to vii. in a data frame for the first document in the corpus. Add a column keys with the feature names. Compare the results with those from sklearn by merging the two data frames. Where do you think that the differences come from?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzFSPXsBqgHD"
      },
      "source": [
        "keys = approach_1.get_feature_names()\n",
        "keys = np.asarray(keys) #converting corpus to np.array\n",
        "df_1 = pd.DataFrame(data=keys) #creating the data frame\n",
        "df_1.rename(columns={0 :'Key'}, inplace=True)\n",
        "df_1.set_index('Key', inplace=True) #setting index as key\n",
        "#adding the rest of the data\n",
        "df_1['A1'], df_1['A2'], df_1['A3'], df_1['B'] = [transform_1[0].toarray()[0], transform_2[0].toarray()[0], transform_3[0].toarray()[0], transform_4[0].toarray()[0]]\n",
        "df_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4b6i30zqgHM",
        "outputId": "5041ac0a-1746-4c2f-cb2d-abee91a3981d"
      },
      "source": [
        "df_2 = pd.DataFrame(data=(dict.fromkeys(corpus_gen[0])).keys()) #creating the data frame\n",
        "df_2.rename(columns={0 :'Key'}, inplace=True)\n",
        "df_2.set_index('Key', inplace=True) #setting index as key\n",
        "#adding the rest of the data\n",
        "df_2['corpus1'] = np.array(corpus1)\n",
        "#df_2['corpus1'] = id2word.doc2bow(corpus_gen[0],allow_update=True)\n",
        "df_2\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Length of values does not match length of index",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-b6fe17d98706>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Key'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#setting index as key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#adding the rest of the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'corpus1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m#df_2['corpus1'] = id2word.doc2bow(corpus_gen[0],allow_update=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdf_2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/tws/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2937\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2938\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2940\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/tws/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2999\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3000\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3001\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/tws/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, key, value, broadcast)\u001b[0m\n\u001b[1;32m   3634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3635\u001b[0m             \u001b[0;31m# turn me into an ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3636\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3637\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3638\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/tws/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36msanitize_index\u001b[0;34m(data, index, copy)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Length of values does not match length of index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCIndexClass\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Length of values does not match length of index"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2l7iC9IqgHP"
      },
      "source": [
        "doc1_df2 = pd.DataFrame(data=(dict.fromkeys(corpus_gen[0])).keys()) \n",
        "doc1_df2.rename(columns={0 :'Key'}, inplace=True)\n",
        "doc1_df2.set_index('Key', inplace=True) #setting index as key\n",
        "#adding the rest of the data\n",
        "doc1_df2['corpus1'] = id2word.doc2bow(corpus_gen[0],allow_update=True)\n",
        "doc1_df['corpus2'] = [[(token[0],(token[1]/sum(n for _, n in doc))) for token in doc] for doc in corpus1] \n",
        "doc1_df2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWFvSBrMqgHR"
      },
      "source": [
        "## 5. Ngrams:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFhhHtsoqgHS"
      },
      "source": [
        "### a. Apply the following transformation to the stemmed data using fit_transform(): \n",
        "    CountVectorizer(ngram_range=(2, 2), max_df=0.95, min_df=0.05). \n",
        "### What is it doing? Print the corresponding features. What are the differences to the features in Question 3. and Question 4? What happens if you change the values of max_df and min_df?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gf893YuiqgHS"
      },
      "source": [
        "n_gram = CountVectorizer(ngram_range=(2, 2), max_df=0.95, min_df=0.05)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRHSTVJ_qgHX",
        "outputId": "bc8c51d2-6ace-40ed-92af-f4dcdc1713f6"
      },
      "source": [
        "transform_6 = n_gram.fit_transform(corpus)\n",
        "feature_names = n_gram.get_feature_names()\n",
        "print(feature_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['articl apr', 'edu write', 'write articl']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ba1boL1bqgHZ"
      },
      "source": [
        "### b. Put the result in a data frame with columns=feature_names. Print the data frame head and the maximum values for each word. What do you notice?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlqRKSu7qgHZ",
        "outputId": "ebad7e1d-4b66-441f-f8d1-f8dcd9827ab6"
      },
      "source": [
        "ngram_features = np.asarray(feature_names) #converting corpus to np.array\n",
        "df_3 = pd.DataFrame(data=ngram_features) #creating the data frame\n",
        "df_3.rename(columns={0: feature_names}, inplace=True)\n",
        "df_3.set_index('feature_names', inplace=True) #setting index as key\n",
        "df_3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "unhashable type: 'list'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.map_locations\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: 'pandas._libs.index.IndexEngine._call_map_locations'\n",
            "Traceback (most recent call last):\n",
            "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1653, in pandas._libs.hashtable.PyObjectHashTable.map_locations\n",
            "TypeError: unhashable type: 'list'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"None of ['feature_names'] are in the columns\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-8285ce16e908>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mngram_features\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#creating the data frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf_3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'feature_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#setting index as key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdf_3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/tws/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mset_index\u001b[0;34m(self, keys, drop, append, inplace, verify_integrity)\u001b[0m\n\u001b[1;32m   4301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4302\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4303\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of {missing} are in the columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4305\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"None of ['feature_names'] are in the columns\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4oAX9_sqgHb"
      },
      "source": [
        "#REFERENCES\n",
        "#https://www.machinelearningplus.com/nlp/gensim-tutorial/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCFOiqTIqgHe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}